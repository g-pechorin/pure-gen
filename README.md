

> Currently this is "Windows Only" due to my lack of not-Windows testing.

> i'm workgin on that

This system demonstrates using [the (pure) functional programming language PureScript](https://www.purescript.org/) to ["script" (in the Unity3D sense)](https://docs.unity3d.com/Manual/ScriptingSection.html) the interactions between the systems that make up an interactive AI.
For the time being - there are only speech recognition and speech synthesis components.

[Installation Instructions](INSTALL.md) provide a guide to setting up the system and running the demonstration.

Interactive AI development is (kind of) an exercise in tying disparate software components together.
These components areusually all "real time" and themselves somewhat experimental.
This scenario leads to a situation where the developer must process various streams of data in a non-blocking manner.
The system here uses [functional reactive programming](https://en.wikipedia.org/wiki/Functional_reactive_programming) to allow someone to construct blocks of logic that tie together and produce the system.

The developer writes an **agent** as a PureScript module which connects to foreign software components and constructs an initial signal function instance.
The framework implements a sort of **shell** which connects some component bindings for this agent and then "cycles" it sending input data and updating outputs.
Data items coming in which cause an update cycle are termed **event**s and data going out denpoting the new state of some component is called **signal**s.

<!-- need more rewrite here -->

From an imperative background;
- the agent is a state machine with an undetermined number of states
- each event triggers the creation of a new instance of the machine
	- as the agent's state is immutable, this is unavoidable
- the agent doesn't directly control outputs, rather it specifies what to output at any given time
	- directly controlling output, as music, would be passing "the current" note out to a synthesizer
	- the approach here passes out a song to play, and when to "have started" playing it

The approach is meant to provide guidance allowing component developers and agent developers to coordinate their efforts in a way that's automatically "checked" using type signatures and the structure of the language.

- System compilation is mostly done "as Scala"
	1. there are `.pidl` interface definition files that generate interface `trait` classes to connect the components to the system
	2. from there, the system (tries to) compile any/all relevant `.scala` to produce an executable

- System startup compiles the PureScript `.purs` files and "lanches" it as a
	1. the same `.pidl` files generate `.purs` and `.js` source code to interact with the connected executable
	2. a library folder (with `Main` and `FRP` modules) and a source folder (with the `Agent` module) are compiled as well (with the generated code) to produce a `.js` bundle
	3. any/all components being used are instantiated
	4. a constructor (from the `Main` module) is used to create the initial instance of the/an agent
	5. the agent's construction "opens" any "foreign signal functions" that connect it to components
		- components can ask for a synchronized object to trigger cycles
	6. the shell enters a loop of execution
		1. all components that send data in are polled for input
		2. a single "cycle" of the agent is executed
		3. all output generated by the cycle is put out to components
		4. the system awaits a signal to run again

Developing these component "drivers" could be a subject in its own.

With this in mind, there should be a tutorial for updating the agent ... which is what I'm working towards.
